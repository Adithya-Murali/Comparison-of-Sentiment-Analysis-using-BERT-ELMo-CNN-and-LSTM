{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Elmo_amazon200k.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWa0-hU30o1k",
        "colab_type": "text"
      },
      "source": [
        "# Import needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C2TtqNLzviH",
        "colab_type": "code",
        "outputId": "81e27bdf-aabd-4481-8dcf-b1815443c511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers\n",
        "from keras.layers import Dense, Embedding, LSTM, Conv1D, Conv2D, MaxPooling1D, Reshape, Flatten, Dropout, CuDNNLSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import operator\n",
        "\n",
        "\n",
        "# Import our dependencies\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "from keras import backend as K\n",
        "import keras.layers as layers\n",
        "from keras.models import Model, load_model\n",
        "from keras.engine import Layer\n",
        "\n",
        "\n",
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "#from appos import appos\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Start with loading all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WxTVgms1AGv",
        "colab_type": "text"
      },
      "source": [
        "# Loading, extracting and pre-processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V7SE2ER0_00",
        "colab_type": "code",
        "outputId": "f7002251-f49e-439c-d368-8c15417cd224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth) \n",
        "\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1B1BHZvMZ8BgfpLkPzsPz7sLGn8lubN7V'\n",
        "\n",
        "fluff,id = link.split('=')\n",
        "print (id)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('Reviews.csv')  \n",
        "data = pd.read_csv('Reviews.csv')\n",
        "data = data[['Text', 'Score']]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0517 11:20:18.889568 140253575722880 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1B1BHZvMZ8BgfpLkPzsPz7sLGn8lubN7V\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2LXc_6j4hs5",
        "colab_type": "text"
      },
      "source": [
        "# Convert the ratings to the sentiments negative and positive.\n",
        "Each sentiment will be represented by integers 0 or 1.\n",
        "\n",
        "0: negative\n",
        "\n",
        "1: positive\n",
        "\n",
        "The ratings and sentiments will be mapped as follows.\n",
        "\n",
        "Rating 1-3 -> negative (0)\n",
        "\n",
        "Rating 4-5 -> positive (1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eSKUA_j0yUs",
        "colab_type": "code",
        "outputId": "427430b9-8663-47ce-ac40-04b9e74129e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "source": [
        "\n",
        "\n",
        "# Change from 1-5 ratings to negative or positive sentiment\n",
        "rating_to_sentiment = { 1: 0, 2: 0, 3: 0, 4: 1, 5: 1 }\n",
        "data['Sentiment'] = data['Score'].apply(lambda x: rating_to_sentiment[x])\n",
        "data=data.sample(200000)\n",
        "\n",
        "# Count number of negative and positive reviews\n",
        "neg_num = pd.value_counts(data['Sentiment'])[0]\n",
        "pos_num = pd.value_counts(data['Sentiment'])[1]\n",
        "\n",
        "print('# negative reviews before: {}'.format(neg_num))\n",
        "print('# positive reviews before: {}'.format(pos_num))\n",
        "\n",
        "# Make the data set balanced\n",
        "balanced_sample_num = np.min([neg_num, pos_num])\n",
        "\n",
        "# Picks <'balanced_sample_num'> numbers of negative and positive reviews at random\n",
        "data = (data.groupby('Sentiment', as_index = False)\n",
        "        .apply(lambda x: x.sample(n = balanced_sample_num))\n",
        "        .reset_index(drop = True))\n",
        "\n",
        "# Shuffle the rows so that 0's and 1's are mixed\n",
        "data = data.sample(frac = 1).reset_index(drop = True)\n",
        "\n",
        "\n",
        "print('\\n# negative reviews after: {}'.format(pd.value_counts(data['Sentiment'])[0]))\n",
        "print('# positive reviews after: {}'.format(pd.value_counts(data['Sentiment'])[1]))\n",
        "print(data['Text'])\n",
        "# Get one-hot encoding for the labels\n",
        "Y = pd.get_dummies(data['Sentiment']).values\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# negative reviews before: 43800\n",
            "# positive reviews before: 156200\n",
            "\n",
            "# negative reviews after: 43800\n",
            "# positive reviews after: 43800\n",
            "0        My baby loves this formula. It does suck that ...\n",
            "1        I love this cereal, and eat it daily.  I used ...\n",
            "2        I am trying to find a little candy that is not...\n",
            "3        I purchased this oil as I was thrilled that a ...\n",
            "4        This kit arrived in a timely manner and has al...\n",
            "5        The coins are hard, they take a day and a half...\n",
            "6        Amazon needs to check their stock, I ordered t...\n",
            "7        i was very disappointed in this soup.  it was ...\n",
            "8        Because I like sweet potatoe fries, I thought ...\n",
            "9        Buyer Beware Please! This sweetener is not for...\n",
            "10       There is another brand of organic rooibos tea ...\n",
            "11       This Product came with outter plastic cover re...\n",
            "12       I started using Crisco on my skin about six mo...\n",
            "13       If you want pumpking flavored coffee buy the T...\n",
            "14       The people who sent the hot sauce or bbq sauce...\n",
            "15       bought this coffee for my work and we all love...\n",
            "16       These treats are made from high quality ingred...\n",
            "17       We tried a box of these K-Cups in our office, ...\n",
            "18       Great selection of different coffee options.  ...\n",
            "19       We have tried every cake mix product there is ...\n",
            "20       I like brewing loose tea and enjoy Davidson's ...\n",
            "21       ive bought a few different kinds and i feel th...\n",
            "22       My little boy (3 1/2 yrs. old) just loves Gold...\n",
            "23       This system is no muss, no fuss and the herbs ...\n",
            "24       Omaha steaks and other so called gourmet food ...\n",
            "25       This stuff is really gross - I tried the lemon...\n",
            "26       This popcorn tastes good, just do not add all ...\n",
            "27       our family did not leke thast at all. Din not ...\n",
            "28       Trehalose is a tasty sugar substitute (tastes ...\n",
            "29       Bought this as an add-on to my main order on a...\n",
            "                               ...                        \n",
            "87570    I have tried the Red White and Blueberry bar a...\n",
            "87571    Thought I would try this tuna after reading th...\n",
            "87572    The Pumpkin spice Latte is another seasonal re...\n",
            "87573    Wow, this tea is the best, my daughter got som...\n",
            "87574    The product description for this tea on Amazon...\n",
            "87575    They should have someone hold this bag and ret...\n",
            "87576    The coffee is good but not great. The package ...\n",
            "87577    My Grandmother use to grow these beans on the ...\n",
            "87578    I love licorice and try to drink pomogranate j...\n",
            "87579    This blend is called Yugen.  It's a very nice ...\n",
            "87580    This is a really good product, the flavor is n...\n",
            "87581    This italian blend is a very pleasent coffee t...\n",
            "87582    I love these snacks and I love even more that ...\n",
            "87583    My Black Lab loves these treats, I need to kee...\n",
            "87584    Decent tasting water, no discernible differenc...\n",
            "87585    When I bought this item the first time, I was ...\n",
            "87586    My midwife recommended this product, and I dra...\n",
            "87587    I was so excited for this to arrive because I ...\n",
            "87588    To quote another reviewer, I can't understand ...\n",
            "87589    Stash teas are really good and this is the bes...\n",
            "87590    Although I like the idea of this product, my c...\n",
            "87591    The aftertaste is tart and metallic - makes wh...\n",
            "87592    I bought this thing after a recommendation fro...\n",
            "87593    I was very diappointed to find there were tran...\n",
            "87594    This product is my wife's favorite.  Kids love...\n",
            "87595    LOVE THEM, BUT ATKIN CARB FREE TASTE A LITTLE ...\n",
            "87596    This is excellent hot chocolate, and I will bu...\n",
            "87597    These are a little different from regular grah...\n",
            "87598    Everyone loves a slim jim.  It doesn't matter ...\n",
            "87599    This is a great product very excited to receiv...\n",
            "Name: Text, Length: 87600, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSMywl587BXt",
        "colab_type": "text"
      },
      "source": [
        "# Perform pre-processing on the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bni7aUYA6_5f",
        "colab_type": "code",
        "outputId": "4f02cdca-6ab1-4d83-d143-8f97241edf55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1071
        }
      },
      "source": [
        "\n",
        "# All characters to lower case\n",
        "data['Text'] = data['Text'].apply(lambda x: x.lower())\n",
        "\n",
        "# # Convert words with apostrophes to its corresponding words, e.g. \"it's\" -> \"it is\"\n",
        "# data['Text'] = data['Text'].apply(lambda x: x.split())\n",
        "# data['Text'] = data['Text'].apply(lambda x: \" \".join([appos[word] if word in appos else word for word in x]))\n",
        "\n",
        "# Remove html-tags, punctuation, commas, numbers etc\n",
        "data['Text'] = data['Text'].apply((lambda x: re.sub('<[^<]+?>', ' ', x)))\n",
        "data['Text'] = data['Text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', ' ', x)))\n",
        "data['Text'] = data['Text'].apply((lambda x: re.sub('^\\d+\\s|\\s\\d+\\s|\\s\\d+$', ' ', x)))\n",
        "\n",
        "# Convert text into tokens, in this case sentences into words\n",
        "data['Text'] = data.apply(lambda x: word_tokenize(x['Text']), axis = 1)\n",
        "\n",
        "# Remove most commonly occuring words which are not relevant in the context of the data\n",
        "irrelevant_words = stopwords.words('english')\n",
        "data['Text'] = data['Text'].apply(lambda x: [word for word in x if word not in irrelevant_words])\n",
        "\n",
        "# Find the base form of the word (lemmatization)\n",
        "lemma = WordNetLemmatizer()\n",
        "data['Text'] = data['Text'].apply(lambda x: \" \".join([lemma.lemmatize(word) for word in x]))\n",
        "print(data['Text'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        baby love formula suck store around sell order...\n",
            "1        love cereal eat daily used buy vanilla creme v...\n",
            "2        trying find little candy bad one aftertaste un...\n",
            "3        purchased oil thrilled local grocer actually c...\n",
            "4        kit arrived timely manner piece necessary buil...\n",
            "5        coin hard take day half chew anise flavor lico...\n",
            "6        amazon need check stock ordered arrived stale ...\n",
            "7        disappointed soup thin oily vegetable little m...\n",
            "8        like sweet potatoe fry thought would like foun...\n",
            "9        buyer beware please sweetener everybody maltit...\n",
            "10       another brand organic rooibos tea offered amaz...\n",
            "11       product came outter plastic cover removed bar ...\n",
            "12       started using crisco skin six month ago produc...\n",
            "13       want pumpking flavored coffee buy timothy perf...\n",
            "14       people sent hot sauce bbq sauce sent glass bot...\n",
            "15       bought coffee work love unusual u like thing g...\n",
            "16       treat made high quality ingredient made u two ...\n",
            "17       tried box k cup office people included get pas...\n",
            "18       great selection different coffee option includ...\n",
            "19       tried every cake mix product gluten free since...\n",
            "20       like brewing loose tea enjoy davidson loose ro...\n",
            "21       ive bought different kind feel great product g...\n",
            "22       little boy 1 yr old love goldfish sure got per...\n",
            "23       system mus fuss herb wonderful black thumb gro...\n",
            "24       omaha steak called gourmet food company people...\n",
            "25       stuff really gross tried lemon lime berry neit...\n",
            "26       popcorn taste good add salt sensitive soft flu...\n",
            "27       family leke thast din atst lke fig sweet enoug...\n",
            "28       trehalose tasty sugar substitute taste like po...\n",
            "29       bought add main order whim basic nutritional i...\n",
            "                               ...                        \n",
            "87570    tried red white blueberry bar really enjoyed s...\n",
            "87571    thought would try tuna reading review price re...\n",
            "87572    pumpkin spice latte another seasonal release m...\n",
            "87573    wow tea best daughter got south mailed found a...\n",
            "87574    product description tea amazon state good eart...\n",
            "87575    someone hold bag retake picture really big bag...\n",
            "87576    coffee good great package supposed vacuum pack...\n",
            "87577    grandmother use grow bean farm ordered bean ba...\n",
            "87578    love licorice try drink pomogranate juice dail...\n",
            "87579    blend called yugen nice matcha usucha even pre...\n",
            "87580    really good product flavor overwhelmingly swee...\n",
            "87581    italian blend pleasent coffee drink anytime fi...\n",
            "87582    love snack love even take airport security pla...\n",
            "87583    black lab love treat need keep hand buy two bo...\n",
            "87584    decent tasting water discernible difference fi...\n",
            "87585    bought item first time happy subscribed saved ...\n",
            "87586    midwife recommended product drank throughout s...\n",
            "87587    excited arrive love orgain drink two benefit o...\n",
            "87588    quote another reviewer understand high praise ...\n",
            "87589          stash tea really good best one really enjoy\n",
            "87590    although like idea product cat hated one picky...\n",
            "87591    aftertaste tart metallic make whatever planned...\n",
            "87592    bought thing recommendation pet store rep dog ...\n",
            "87593    diappointed find trans fat cooky ginger one fa...\n",
            "87594    product wife favorite kid love would suggest l...\n",
            "87595    love atkin carb free taste little better still...\n",
            "87596    excellent hot chocolate buy recommend highly g...\n",
            "87597    little different regular graham cracker bit th...\n",
            "87598    everyone love slim jim matter old great gift i...\n",
            "87599    great product excited receive love convenience...\n",
            "Name: Text, Length: 87600, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6IyQMOe5VnWC"
      },
      "source": [
        "Split data-set into 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFZABApc0x9f",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR744ZdqDaji",
        "colab_type": "code",
        "outputId": "6f4abf54-f7a1-4aad-989b-2cb6a7608b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\"\"\"smaller_set_percentage = 0.5\n",
        "split = int(round(smaller_set_percentage * data.shape[0]))\n",
        "\n",
        "X_small = data['Text'][0:split]\n",
        "Y_small = Y[0:split]\n",
        "\n",
        "X_large = data['Text'][split:data.shape[0]]\n",
        "Y_large = Y[split:data.shape[0]]\n",
        "print()\n",
        "print('Small data set: X: {} Y: {}'.format(X_small.shape, Y_small.shape))\n",
        "print('Large data set: X: {} Y: {}'.format(X_large.shape, Y_large.shape)) \"\"\"\n",
        "\n",
        "X=data\n",
        "test_size, val_size = 0.2, 0.05\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_size)\n",
        "\n",
        "print('X train: {}, Y train: {}'.format(X_train.shape, Y_train.shape))\n",
        "print('X test: {}, Y test: {}'.format(X_test.shape, Y_test.shape))\n",
        "\n",
        "train_x=X_train\n",
        "test_x=X_test\n",
        "train_y=Y_train\n",
        "test_y=Y_test\n",
        "\n",
        "\n",
        "\"\"\"smaller_set_percentage = 0.5\n",
        "split = int(round(smaller_set_percentage * data.shape[0]))\n",
        "\n",
        "train_x=data[split:]\n",
        "test_x=data[:split]\n",
        "train_y=Y[split:]\n",
        "test_y=Y[:split]\n",
        "\n",
        "print(train_x)\n",
        "print(test_y)\n",
        "print('training data set: X: {} Y: {}'.format(train_x.shape, train_y.shape))\n",
        "print('testing data set: X: {} Y: {}'.format(test_x.shape, test_y.shape))\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X train: (70080, 3), Y train: (70080, 2)\n",
            "X test: (17520, 3), Y test: (17520, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"smaller_set_percentage = 0.5\\nsplit = int(round(smaller_set_percentage * data.shape[0]))\\n\\ntrain_x=data[split:]\\ntest_x=data[:split]\\ntrain_y=Y[split:]\\ntest_y=Y[:split]\\n\\nprint(train_x)\\nprint(test_y)\\nprint('training data set: X: {} Y: {}'.format(train_x.shape, train_y.shape))\\nprint('testing data set: X: {} Y: {}'.format(test_x.shape, test_y.shape))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO83B6UEYHZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a custom layer that allows us to update weights (lambda layers do not have trainable parameters!)\n",
        "\n",
        "class ElmoEmbeddingLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.dimensions = 1024\n",
        "        self.trainable=True\n",
        "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
        "                               name=\"{}_module\".format(self.name))\n",
        "\n",
        "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
        "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
        "                      as_dict=True,\n",
        "                      signature='default',\n",
        "                      )['default']\n",
        "        return result\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return K.not_equal(inputs, '--PAD--')\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.dimensions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXN8JDH7LHy0",
        "colab_type": "code",
        "outputId": "60946947-ac5c-4a3e-df42-b2dcdcddad82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2043
        }
      },
      "source": [
        "!pip install allennlp\n",
        "\n",
        "from allennlp.training.metrics.metric import Metric\n",
        "from allennlp.training.metrics import f1_measure\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: allennlp in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: numpydoc>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.9.1)\n",
            "Requirement already satisfied: parsimonious>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.8.1)\n",
            "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
            "Requirement already satisfied: flaky in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.5.3)\n",
            "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.6)\n",
            "Requirement already satisfied: word2number>=1.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.6.2)\n",
            "Requirement already satisfied: tensorboardX>=1.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.6)\n",
            "Requirement already satisfied: flask-cors>=3.0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
            "Requirement already satisfied: jsonnet>=0.10.0; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.12.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.2.1)\n",
            "Requirement already satisfied: awscli>=1.11.91 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.160)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
            "Requirement already satisfied: spacy<2.2,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.18)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
            "Requirement already satisfied: conllu==0.11 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.0.3)\n",
            "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
            "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
            "Requirement already satisfied: responses>=0.7 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.10.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from allennlp) (5.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.16.3)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9.147)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.0.23)\n",
            "Requirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.20.3)\n",
            "Requirement already satisfied: moto>=1.3.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.8)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.10.1)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2018.1.10)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.7.1)\n",
            "Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n",
            "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.4.2)\n",
            "Requirement already satisfied: colorama<=0.3.9,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.3.9)\n",
            "Collecting botocore==1.12.150 (from awscli>=1.11.91->allennlp)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/a3/14582589522f21684726da7b0f57dc2258cc6e3adb6046675f3bd9eba834/botocore-1.12.150-py2.py3-none-any.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.14)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (0.2.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.1)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.2.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.0.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (1.35)\n",
            "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (6.12.1)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (0.9.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.0->allennlp) (2.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.5.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
            "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.15.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (41.0.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (7.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.1.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.7)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (0.12.0)\n",
            "Requirement already satisfied: boto>=2.36.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.49.0)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (3.0.5)\n",
            "Requirement already satisfied: cryptography>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.6.1)\n",
            "Requirement already satisfied: jsondiff==1.1.2 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (1.1.2)\n",
            "Requirement already satisfied: cfn-lint in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (0.20.3)\n",
            "Requirement already satisfied: python-jose<4.0.0 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (3.0.1)\n",
            "Requirement already satisfied: docker>=2.5.1 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (3.7.2)\n",
            "Requirement already satisfied: aws-xray-sdk!=0.96,>=0.93 in /usr/local/lib/python3.6/dist-packages (from moto>=1.3.4->allennlp) (2.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (19.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.6.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.5)\n",
            "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.4.3.2)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0.1)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (1.12.3)\n",
            "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.3.0->moto>=1.3.4->allennlp) (0.24.0)\n",
            "Requirement already satisfied: jsonschema~=2.6 in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->allennlp) (2.6.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->allennlp) (1.23)\n",
            "Requirement already satisfied: aws-sam-translator>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cfn-lint->moto>=1.3.4->allennlp) (1.11.0)\n",
            "Requirement already satisfied: ecdsa<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<4.0.0->moto>=1.3.4->allennlp) (0.13.2)\n",
            "Requirement already satisfied: future<1.0 in /usr/local/lib/python3.6/dist-packages (from python-jose<4.0.0->moto>=1.3.4->allennlp) (0.16.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.4.0)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from docker>=2.5.1->moto>=1.3.4->allennlp) (0.56.0)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.6/dist-packages (from aws-xray-sdk!=0.96,>=0.93->moto>=1.3.4->allennlp) (1.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.1->spacy<2.2,>=2.0->allennlp) (0.9.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.3.0->moto>=1.3.4->allennlp) (2.19)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.6/dist-packages (from jsonpatch->cfn-lint->moto>=1.3.4->allennlp) (2.0)\n",
            "Installing collected packages: botocore\n",
            "  Found existing installation: botocore 1.12.147\n",
            "    Uninstalling botocore-1.12.147:\n",
            "      Successfully uninstalled botocore-1.12.147\n",
            "Successfully installed botocore-1.12.150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "botocore"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROZTNo-YJa5",
        "colab_type": "code",
        "outputId": "e6d0088e-4424-4edf-bfb1-37a69fe21849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Function to build model\n",
        "!pip install keras_metrics\n",
        "import keras\n",
        "import keras_metrics\n",
        "def build_model(): \n",
        "  input_text = layers.Input(shape=(1,), dtype=\"string\")\n",
        "  embedding = ElmoEmbeddingLayer()(input_text)\n",
        "  dense = layers.Dense(256, activation='relu')(embedding)\n",
        "  pred = layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "  model = Model(inputs=[input_text], outputs=pred)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        " \n",
        "  model.summary()\n",
        "  #,keras_metrics.precision(), keras_metrics.recall()\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.2.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.7)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.16.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aDfsnk_YVin",
        "colab_type": "code",
        "outputId": "7cffc97a-aaff-499a-b548-d3dfd0868082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Create datasets (Only take up to 150 words for memory)\n",
        "\n",
        "train_text = train_x['Text'].tolist()\n",
        "train_text = [' '.join(t.split()[0:150]) for t in train_text]\n",
        "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
        "train_label = train_x['Sentiment'].tolist()\n",
        "print(train_text)\n",
        "\n",
        "test_text = test_x['Text'].tolist()\n",
        "test_text = [' '.join(t.split()[0:150]) for t in test_text]\n",
        "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
        "test_label = test_x['Sentiment'].tolist()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['thick strong likely adding soup rice']\n",
            " ['wonderful product dog really love one thing make product better chew box would one every day month']\n",
            " ['movie always popular different entertaining fast moving hour half film available bllu ray look better nothing outstanding definite improvement shapnress several character mainly michael keaton unique sometimes revolting title character beetlejuice always fascinating watch whole movie also get lot humor scary special effect odd see alec baldwin low key role 90 played type guy davis look act like well davis many time played nice people viewer like took four viewing finally appreciated catharine hara comedic talent movie favorite someone find absolutely hilarious messed wife mother family move haunted house inhabited baldwin davis keaton made name actor whacked robin williams type role although never really followed anything popular film winona rider cute teenage daughter get fun supporting role diverse people talk show host dick cavett singer robert goulet actor jeffrey jones good tim burton directed film surprised typical occult theme ghost like heaven hell strange existence touted dead people go ridiculous']\n",
            " ...\n",
            " ['huge peanut butter fan usually force eat work dont time cook saw planter sale tried know much better buy brand eating type nom nom']\n",
            " ['boyfriend cat eating food six year switched cat developed uti urinating house also gained ton weight review found online show cat owner similar problem cat getting sick eating food please use caution could make cat ill cat eat food always keep eye always thought male cat prone utis yet female cat sick cost vet bill feeding food']\n",
            " ['enabler two dog insatiable addiction greenies every night around 45 p start whining barking circling stop get greenie day consistent buyer greenies like know get expensive local chain pet store expect pay 99 per bag greenies oz teenie piece lead review amazon performance delivering doggie treat reasonable price currently 99 per bag buy bag get one free addition free shipping save time needle trip pet store buy bag getting per bag fantastic deal term snack senior version teenie greenies great taste great original based dog ability consume minute softer consistency one dog nearly year old year old dental care stressing teeth really important long amazon maintains deal continue purchase item directly']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_O_AyqzZoJf",
        "colab_type": "code",
        "outputId": "28a8c0d9-369c-44b8-bb20-0f11a564fdd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "# Build and fit\n",
        "val_size=0.2\n",
        "model = build_model()\n",
        "model.fit(train_text, \n",
        "          train_label,\n",
        "          validation_split=val_size,\n",
        "          epochs=4,\n",
        "          batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0517 14:44:49.580165 140253575722880 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "elmo_embedding_layer_4 (Elmo (None, 1024)              4         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 262,661\n",
            "Trainable params: 262,661\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 56064 samples, validate on 14016 samples\n",
            "Epoch 1/4\n",
            "56064/56064 [==============================] - 1198s 21ms/step - loss: 0.4884 - acc: 0.7647 - val_loss: 0.4738 - val_acc: 0.7722\n",
            "Epoch 2/4\n",
            "56064/56064 [==============================] - 1191s 21ms/step - loss: 0.4461 - acc: 0.7882 - val_loss: 0.4247 - val_acc: 0.8049\n",
            "Epoch 3/4\n",
            "56064/56064 [==============================] - 1186s 21ms/step - loss: 0.4289 - acc: 0.7991 - val_loss: 0.4836 - val_acc: 0.7703\n",
            "Epoch 4/4\n",
            "56064/56064 [==============================] - 1190s 21ms/step - loss: 0.4153 - acc: 0.8075 - val_loss: 0.4425 - val_acc: 0.7880\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8eb0e49518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I_JBAiwZNej",
        "colab_type": "code",
        "outputId": "1a174073-60ab-41bc-a9bb-5cd42b5b89b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\"model.save('ElmoModel.h5')\n",
        "pre_save_preds = model.predict(test_text[0:100]) # predictions before we clear and reload model\n",
        "\n",
        "# Clear and load model\n",
        "model = None\n",
        "model = build_model()\n",
        "model.load_weights('ElmoModel.h5')\n",
        "\n",
        "post_save_preds = model.predict(test_text[0:100]) # predictions after we clear and reload model\n",
        "all(pre_save_preds == post_save_preds) # Are they the same?\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"model.save(\\'ElmoModel.h5\\')\\npre_save_preds = model.predict(test_text[0:100]) # predictions before we clear and reload model\\n\\n# Clear and load model\\nmodel = None\\nmodel = build_model()\\nmodel.load_weights(\\'ElmoModel.h5\\')\\n\\npost_save_preds = model.predict(test_text[0:100]) # predictions after we clear and reload model\\nall(pre_save_preds == post_save_preds) # Are they the same?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HfprDT91hYi_",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dVdHIVLCbOY",
        "colab_type": "code",
        "outputId": "f7c5c7e1-508f-4db3-ad87-5fc424ff587a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1122
        }
      },
      "source": [
        "print(X_test)\n",
        "Y_test_new=Y_test[:,1]\n",
        "print(Y_test_new.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    Text  Score  Sentiment\n",
            "45454  expecting coffee actually shipped jamaica plea...      5          1\n",
            "25109  returning item came white baggie box packaging...      2          0\n",
            "9629   like tea strong tea delivers let brew longer 7...      5          1\n",
            "9799   think sitting around warehouse long time took ...      1          0\n",
            "32012  friend bought tea tjmaxx box contained bag fin...      5          1\n",
            "8932   beagle lab mix fairly picky treat go nut tryin...      5          1\n",
            "50203  ordered product expecting receive whole seed p...      1          0\n",
            "62542  nice able buy quality organic sugar good price...      5          1\n",
            "77138  used drink time great coffee recently noticed ...      2          0\n",
            "35540  title review suggests thought jerky little rub...      3          0\n",
            "63826  big fan emerald mixed nut plastic lately offer...      1          0\n",
            "64228  coffee tasted bitter like burnt cleaned machin...      1          0\n",
            "39074  tea good smell yummy highly recommend really l...      4          1\n",
            "68448  tried night fantastic easy make add water lett...      5          1\n",
            "66217  delicious memory booster eating newton many ch...      5          1\n",
            "16719  purely chance first taste single origin chocol...      5          1\n",
            "37520  love chip little weird review potato chip chip...      5          1\n",
            "55939  love molasses much fair trade organic molasses...      5          1\n",
            "44349  love snapea crisp original like caesar flavore...      3          0\n",
            "53077  plant arrived great condition fully green five...      5          1\n",
            "12724  tried brand plain salmon jerky reasonably good...      1          0\n",
            "2360   product horid moldy mess unbelievable could ev...      1          0\n",
            "6712   review make sound really stupid whatever reall...      1          0\n",
            "37973  kitten love food knew take little bit prepare ...      5          1\n",
            "54990  mini schnauzer month im alot training though b...      3          0\n",
            "75670  kiwaii water new zealand begin say enough good...      5          1\n",
            "69624  really excited learn product everything say sw...      3          0\n",
            "13565  always loved tea general prefer drink tea thin...      5          1\n",
            "9104   thorough research infinite type protein bar de...      5          1\n",
            "75099  worst thing ever ingested sold super low calor...      1          0\n",
            "...                                                  ...    ...        ...\n",
            "41822  u grew u space age use item like way introduce...      4          1\n",
            "61306  candy awful love candy wanted something new ha...      1          0\n",
            "51417  nothing product opinion left dog two small dog...      3          0\n",
            "87014  get fresh batch stuff amazing fresh like right...      5          1\n",
            "38688  excellent proplan dry cat food work well prote...      5          1\n",
            "79561  chocolate taste definitely along oatmeal raisi...      3          0\n",
            "26822  used baking father baker passing used almond p...      4          1\n",
            "41188  drink coffee black prefer bolder stronger k cu...      5          1\n",
            "58704  pretty sure sauce use taco bell baja gordita c...      5          1\n",
            "10132  amazing product bought year ago really liked w...      5          1\n",
            "20449  usually coffee first thing morning wake burst ...      5          1\n",
            "63730  started using product month ago read giving co...      5          1\n",
            "56041  decided review product need fiber diet kashi b...      2          0\n",
            "70019  tried peach almond morning love wait try flavo...      5          1\n",
            "29386  coffee way strong smell taste like southern pe...      1          0\n",
            "59143  30oz per find somewhere cheaper later feel kin...      2          0\n",
            "17454  dog picky eater eat dog food would get refused...      5          1\n",
            "66460  bought friday sale really beat price especiall...      5          1\n",
            "70634  mentos watermelon gum fairly good tasting suga...      3          0\n",
            "54779  bought product love decided buy much flour fut...      5          1\n",
            "71056  basically like oreo like taste texture well or...      2          0\n",
            "5759   son undergo physiological performance evaluati...      5          1\n",
            "59676  good pastrami worth trying however good pastra...      3          0\n",
            "46928  although dog like chew rawhide like chewed one...      2          0\n",
            "18868  tried croustades although fantastic good enoug...      3          0\n",
            "20623  old wisconsin turkey snack bite healthy delici...      5          1\n",
            "70667  bought item loving way tasted 2 ounce package ...      1          0\n",
            "57529  buying toro brand admittedly spoiled little da...      2          0\n",
            "69197  wife really enjoy coffee meal dessert unlike k...      5          1\n",
            "22409  latest spin semi homemade dinner kraft velveet...      5          1\n",
            "\n",
            "[17520 rows x 3 columns]\n",
            "(17520,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCyjqnSNPVFV",
        "colab_type": "code",
        "outputId": "a3eb2354-48d1-4f99-85ef-ae4e3b2f42ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "\n",
        "# predict probabilities for test set\n",
        "yhat_probs = model.predict(X_test['Text'], verbose=1)\n",
        "# predict crisp classes for test set\n",
        "labels = (yhat_probs > 0.5).astype(np.int)\n",
        "print(labels)\n",
        "\n",
        "#yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1696/17520 [=>............................] - ETA: 6:24"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-edb45d768bbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myhat_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# predict crisp classes for test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myhat_probs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,1024,1068,44] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node elmo_embedding_layer_4/elmo_embedding_layer_4_module_apply_default/bilm/CNN/Conv2D_6}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node dense_8/Sigmoid}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLr8gPd1QaH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(yhat_probs)\n",
        "print(Y_test_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R0inQuxbqdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#test_label1= np.array(test_label)\n",
        "#print(test_label1)\n",
        "#print(test_label1.shape)\n",
        "#Y_test_new=test_label1[:,1]\n",
        "\n",
        "print(\"elmo_model\")\n",
        "accuracy = accuracy_score(Y_test_new,labels)\n",
        "print('Accuracy: %f' % accuracy)\n",
        "# precision tp / (tp + fp)\n",
        "precision = precision_score(Y_test_new, labels)\n",
        "print('Precision: %f' % precision)\n",
        "# recall: tp / (tp + fn)\n",
        "recall = recall_score(Y_test_new,labels)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(Y_test_new,labels)\n",
        "print('F1 score: %f \\n\\n' % f1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
